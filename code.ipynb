{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.11.0-py3-none-any.whl (468 kB)\n",
      "     -------------------------------------- 468.7/468.7 kB 2.7 MB/s eta 0:00:00\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\alicia\\anaconda3\\lib\\site-packages (from datasets) (2022.7.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\alicia\\anaconda3\\lib\\site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\alicia\\anaconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\alicia\\anaconda3\\lib\\site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\alicia\\anaconda3\\lib\\site-packages (from datasets) (1.21.5)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.2.0-cp39-cp39-win_amd64.whl (30 kB)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
      "     -------------------------------------- 132.9/132.9 kB 4.0 MB/s eta 0:00:00\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.4-cp39-cp39-win_amd64.whl (323 kB)\n",
      "     -------------------------------------- 323.6/323.6 kB 6.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas in c:\\users\\alicia\\anaconda3\\lib\\site-packages (from datasets) (1.4.4)\n",
      "Collecting pyarrow>=8.0.0\n",
      "  Downloading pyarrow-11.0.0-cp39-cp39-win_amd64.whl (20.6 MB)\n",
      "     ---------------------------------------- 20.6/20.6 MB 6.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\alicia\\anaconda3\\lib\\site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\alicia\\anaconda3\\lib\\site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in c:\\users\\alicia\\anaconda3\\lib\\site-packages (from datasets) (0.13.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\alicia\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\alicia\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\alicia\\anaconda3\\lib\\site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alicia\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\alicia\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\alicia\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alicia\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.9.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\alicia\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.5)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp39-cp39-win_amd64.whl (28 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp39-cp39-win_amd64.whl (34 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\alicia\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (21.4.0)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.2-cp39-cp39-win_amd64.whl (56 kB)\n",
      "     ---------------------------------------- 56.8/56.8 kB 3.1 MB/s eta 0:00:00\n",
      "Collecting dill<0.3.7,>=0.3.0\n",
      "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
      "     -------------------------------------- 110.5/110.5 kB 3.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\alicia\\anaconda3\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\alicia\\anaconda3\\lib\\site-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\alicia\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, pyarrow, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.4\n",
      "    Uninstalling dill-0.3.4:\n",
      "      Successfully uninstalled dill-0.3.4\n",
      "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.11.0 dill-0.3.6 frozenlist-1.3.3 multidict-6.0.4 multiprocess-0.70.14 pyarrow-11.0.0 responses-0.18.0 xxhash-3.2.0 yarl-1.8.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets\n",
    "pip install keybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset duc2001/generation to C:/Users/Alicia/.cache/huggingface/datasets/midas___duc2001/generation/0.0.1/7888b46165d8a58f49f00e28410b46b1f22fabfd72a9e89f3e80a4e2d27e4a9b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f350e0e65e1f43a2a89b78554fdd908e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e3803d71c0f4c6580b5872c1fb7ab18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22ebe1d9e27a4a3aa3e7540f9c9d398a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset duc2001 downloaded and prepared to C:/Users/Alicia/.cache/huggingface/datasets/midas___duc2001/generation/0.0.1/7888b46165d8a58f49f00e28410b46b1f22fabfd72a9e89f3e80a4e2d27e4a9b. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183f8bb19bc147b081440d5f6752e7b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples for Keyphrase Generation\n",
      "Sample from test data split\n",
      "Fields in the sample:  ['id', 'document', 'extractive_keyphrases', 'abstractive_keyphrases']\n",
      "Tokenized Document:  ['Here', ',', 'at', 'a', 'glance', ',', 'are', 'developments', 'today', 'involving', 'the', 'crash', 'of', 'Pan', 'American', 'World', 'Airways', 'Flight', '103', 'Wednesday', 'night', 'in', 'Lockerbie', ',', 'Scotland', ',', 'that', 'killed', 'all', '259', 'people', 'aboard', 'and', 'more', 'than', '20', 'people', 'on', 'the', 'ground', ':']\n",
      "Extractive/present Keyphrases:  ['pan american world airways flight 103', 'crash', 'lockerbie']\n",
      "Abstractive/absent Keyphrases:  ['terrorist threats', 'widespread wreckage', 'radical palestinian faction', 'terrorist bombing', 'bomb threat', 'sabotage']\n",
      "\n",
      "-----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the dataset only for keyphrase generation\n",
    "dataset = load_dataset(\"midas/duc2001\", \"generation\")\n",
    "\n",
    "print(\"Samples for Keyphrase Generation\")\n",
    "\n",
    "# sample from the test split\n",
    "print(\"Sample from test data split\")\n",
    "test_sample = dataset[\"test\"][0]\n",
    "print(\"Fields in the sample: \", [key for key in test_sample.keys()])\n",
    "print(\"Tokenized Document: \", test_sample[\"document\"])\n",
    "print(\"Extractive/present Keyphrases: \", test_sample[\"extractive_keyphrases\"])\n",
    "print(\"Abstractive/absent Keyphrases: \", test_sample[\"abstractive_keyphrases\"])\n",
    "print(\"\\n-----------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "356a3a44645c4fa1be6fda891337d6d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/308 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def drop_column(dataset):\n",
    "    del dataset['abstractive_keyphrases']\n",
    "    return dataset\n",
    "\n",
    "# Para eliminar la columna que no necesitamos\n",
    "dataset = dataset.map(drop_column)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasta aquí tenemos un dataset con las 3 columnas de interés:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['id', 'document', 'extractive_keyphrases'],\n",
       "        num_rows: 308\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación juntamos las palabras de 'document' porque estan separadas por comas, y para cada texto resultante implementamos keybert. Guardamos las keywords y el score de cada texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "kw_model = KeyBERT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = []\n",
    "for i in range(len(dataset['test'])):\n",
    "    joined_text = \" \".join(dataset['test'][i]['document'])\n",
    "    keywords_row = []\n",
    "    for keyword_info in kw_model.extract_keywords(joined_text, keyphrase_ngram_range=(1, 1), stop_words=None):\n",
    "        keyword, score = keyword_info  # extraer palabra clave y puntuación\n",
    "        keywords_row.append({'keyword': keyword, 'score': score}) # guardar información en un diccionario\n",
    "    keywords.append(keywords_row) # guardar información de palabras clave para cada fila\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'keyword': 'scott', 'score': 0.3601},\n",
       " {'keyword': 'justice', 'score': 0.3453},\n",
       " {'keyword': 'missouri', 'score': 0.3439},\n",
       " {'keyword': 'unconstitutional', 'score': 0.3265},\n",
       " {'keyword': 'constitutional', 'score': 0.3133}]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords[0] #prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(keywords)==len(dataset['test']) #comprovación para ver que se ha hecho para todos los textos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si miramos las extractive keyphrases, vemos que la mayoría de ellas tienen 1 o 2 N-gramas. Si ahora en vez de keywords queremos extraer kephrases de 2 palabras tenemos que cambiar el parámetro keyphrase_ngram_range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyphrases = []\n",
    "for i in range(len(dataset['test'])):\n",
    "    joined_text = \" \".join(dataset['test'][i]['document'])\n",
    "    keywords_row = []\n",
    "    for keyword_info in kw_model.extract_keywords(joined_text, keyphrase_ngram_range=(1, 2), stop_words=None):\n",
    "        keyword, score = keyword_info  # extraer palabra clave y puntuación\n",
    "        keywords_row.append({'keyword': keyword, 'score': score}) # guardar información en un diccionario\n",
    "    keyphrases.append(keywords_row) # guardar información de palabras clave para cada fila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'keyword': 'flight 103', 'score': 0.4514},\n",
       " {'keyword': 'the crash', 'score': 0.4259},\n",
       " {'keyword': 'world airways', 'score': 0.4206},\n",
       " {'keyword': '259 people', 'score': 0.4203},\n",
       " {'keyword': 'airways flight', 'score': 0.4154}]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyphrases[0] #prueba"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de observar que se puede modificar el tamaño de N-gramas, keybert nos permite modificar otras opciones. Una de  ellas es la Max Sum Distance, la cual coge todas las top_n combinaciones de las 2x top_n palabras que hay, y extrae las combinaciones que son menos similares entre ellas a partir de la similitud de cosinus. Ej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyphrases2 = []\n",
    "for i in range(len(dataset['test'])):\n",
    "    joined_text = \" \".join(dataset['test'][i]['document'])\n",
    "    keywords_row = []\n",
    "    for keyword_info in kw_model.extract_keywords(joined_text, keyphrase_ngram_range=(1, 2), stop_words=None, use_maxsum=True, \n",
    "                                                  nr_candidates=20, top_n=5):\n",
    "        keyword, score = keyword_info  # extraer palabra clave y puntuación\n",
    "        keywords_row.append({'keyword': keyword, 'score': score}) # guardar información en un diccionario\n",
    "    keyphrases2.append(keywords_row) # guardar información de palabras clave para cada fila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'keyword': 'lockerbie scotland', 'score': 0.2471},\n",
       " {'keyword': 'pan american', 'score': 0.2887},\n",
       " {'keyword': 'killed all', 'score': 0.3371},\n",
       " {'keyword': 'airways flight', 'score': 0.4154},\n",
       " {'keyword': '259 people', 'score': 0.4203}]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyphrases2[0] #prueba"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez añadidos estos parámetros, también podemos incorporar el uso de Maximal Marginal Relevance, que sirve para crear keywords/keyphrases y también se basa en la similitud de cosinus. Sirve para diversificar los resultados. Podemos probar con alta diversidad (0.7) o con baja diversidad (0.2). Ej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyphrases3 = []\n",
    "for i in range(len(dataset['test'])):\n",
    "    joined_text = \" \".join(dataset['test'][i]['document'])\n",
    "    keywords_row = []\n",
    "    for keyword_info in kw_model.extract_keywords(joined_text, keyphrase_ngram_range=(1, 2), stop_words=None, use_maxsum=True, \n",
    "                                                  nr_candidates=20, top_n=5, diversity=0.7):\n",
    "        keyword, score = keyword_info  # extraer palabra clave y puntuación\n",
    "        keywords_row.append({'keyword': keyword, 'score': score}) # guardar información en un diccionario\n",
    "    keyphrases3.append(keywords_row) # guardar información de palabras clave para cada fila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'keyword': 'lockerbie scotland', 'score': 0.2471},\n",
       " {'keyword': 'pan american', 'score': 0.2887},\n",
       " {'keyword': 'killed all', 'score': 0.3371},\n",
       " {'keyword': 'airways flight', 'score': 0.4154},\n",
       " {'keyword': '259 people', 'score': 0.4203}]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyphrases3[0] #prueba, vemos que salen las mismas q antes (almenos en este caso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'keyword': 'officials about', 'score': 0.3666},\n",
       " {'keyword': 'boeing 737s', 'score': 0.3767},\n",
       " {'keyword': 'crashed sunday', 'score': 0.3788},\n",
       " {'keyword': 'ordered inspections', 'score': 0.3837},\n",
       " {'keyword': 'engine warning', 'score': 0.3888}]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyphrases2[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'keyword': 'officials about', 'score': 0.3666},\n",
       " {'keyword': 'boeing 737s', 'score': 0.3767},\n",
       " {'keyword': 'crashed sunday', 'score': 0.3788},\n",
       " {'keyword': 'ordered inspections', 'score': 0.3837},\n",
       " {'keyword': 'engine warning', 'score': 0.3888}]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyphrases3[10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
